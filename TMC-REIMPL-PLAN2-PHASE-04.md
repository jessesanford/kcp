# TMC Reimplementation Plan 2 - Phase 4: Advanced Placement & Performance

## üéØ **ARCHITECTURAL FOUNDATION**

**Advanced Placement Logic with KCP Workspace Integration**

- **KCP Role**: Provides workspace-aware placement APIs and resource capacity tracking
- **TMC Controllers**: Implement intelligent placement algorithms based on cluster capabilities
- **Integration**: Full integration with KCP workspace isolation and multi-tenancy
- **Performance**: Optimized for high-throughput, multi-cluster scenarios

## üìã **Phase 4 Objectives**

**Implement production-ready placement algorithms and performance optimization**

- Add advanced placement algorithms (affinity, capacity-based, health-aware)
- Implement cluster capacity management and resource tracking
- Build workspace-aware placement decisions
- Add performance optimization and caching
- **Scope**: 1200-1500 lines across 2 PRs

## üèóÔ∏è **Advanced Placement Architecture**

### **Understanding KCP Workspace Integration**

```go
// Advanced TMC Placement Flow:
// 1. WorkloadPlacement resources define placement policies per workspace
// 2. ClusterRegistration resources track cluster capabilities and capacity
// 3. TMC Placement Engine evaluates constraints and makes decisions
// 4. Placement decisions respect workspace isolation
// 5. Capacity tracking prevents cluster oversubscription
```

**Placement Principles:**
1. **Workspace isolation** - placement decisions respect workspace boundaries
2. **Capacity awareness** - track cluster resource utilization
3. **Health-based decisions** - unhealthy clusters are excluded
4. **Policy-driven** - placement follows defined policies
5. **Performance optimized** - efficient decision making at scale

## üìä **PR 7: Advanced Placement Engine (~800 lines)**

**Objective**: Implement sophisticated placement algorithms with workspace integration

### **Files Created:**
```
pkg/tmc/placement/engine.go                     (~250 lines)
pkg/tmc/placement/capacity.go                   (~200 lines)
pkg/tmc/placement/algorithms.go                 (~200 lines)
pkg/tmc/placement/scheduler.go                  (~150 lines)
```

### **Placement Engine:**
```go
// pkg/tmc/placement/engine.go
package placement

import (
    "context"
    "fmt"
    "sort"
    "sync"
    "time"
    
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/apimachinery/pkg/labels"
    "k8s.io/client-go/tools/cache"
    "k8s.io/klog/v2"
    
    "github.com/kcp-dev/logicalcluster/v3"
    
    tmcv1alpha1 "github.com/kcp-dev/kcp/pkg/apis/tmc/v1alpha1"
    kcpclientset "github.com/kcp-dev/kcp/sdk/client/clientset/versioned/cluster"
    tmcv1alpha1informers "github.com/kcp-dev/kcp/sdk/client/informers/externalversions/tmc/v1alpha1"
    conditionsv1alpha1 "github.com/kcp-dev/kcp/sdk/apis/third_party/conditions/apis/conditions/v1alpha1"
    "github.com/kcp-dev/kcp/sdk/apis/third_party/conditions/util/conditions"
)

// PlacementEngine makes intelligent placement decisions
type PlacementEngine struct {
    // KCP clients
    kcpClusterClient kcpclientset.ClusterInterface
    
    // Listers
    clusterRegistrationLister tmcv1alpha1informers.ClusterRegistrationClusterLister
    workloadPlacementLister   tmcv1alpha1informers.WorkloadPlacementClusterLister
    
    // Components
    capacityManager *CapacityManager
    scheduler       *Scheduler
    
    // Cache for placement decisions
    placementCache *PlacementCache
    
    // Configuration
    workspace logicalcluster.Name
    
    // Synchronization
    mu sync.RWMutex
}

// PlacementDecision represents a placement decision
type PlacementDecision struct {
    TargetClusters []ClusterSelection
    Strategy       tmcv1alpha1.PlacementStrategy
    Timestamp      time.Time
    Reason         string
}

// ClusterSelection represents a selected cluster with scoring
type ClusterSelection struct {
    ClusterName string
    Score       int64
    Reason      string
    Capacity    ClusterCapacity
}

// PlacementCache caches placement decisions for performance
type PlacementCache struct {
    decisions map[string]*PlacementDecision
    ttl       time.Duration
    mu        sync.RWMutex
}

// NewPlacementEngine creates a new placement engine
func NewPlacementEngine(
    kcpClusterClient kcpclientset.ClusterInterface,
    clusterRegistrationInformer tmcv1alpha1informers.ClusterRegistrationClusterInformer,
    workloadPlacementInformer tmcv1alpha1informers.WorkloadPlacementClusterInformer,
    workspace logicalcluster.Name,
) (*PlacementEngine, error) {
    
    capacityManager, err := NewCapacityManager(
        kcpClusterClient,
        clusterRegistrationInformer,
        workspace,
    )
    if err != nil {
        return nil, fmt.Errorf("failed to create capacity manager: %w", err)
    }
    
    scheduler := NewScheduler()
    
    placementCache := &PlacementCache{
        decisions: make(map[string]*PlacementDecision),
        ttl:       5 * time.Minute,
    }
    
    engine := &PlacementEngine{
        kcpClusterClient:          kcpClusterClient,
        clusterRegistrationLister: clusterRegistrationInformer.Lister(),
        workloadPlacementLister:   workloadPlacementInformer.Lister(),
        capacityManager:           capacityManager,
        scheduler:                 scheduler,
        placementCache:           placementCache,
        workspace:                workspace,
    }
    
    return engine, nil
}

// MakePlacementDecision makes a placement decision for a workload
func (e *PlacementEngine) MakePlacementDecision(
    ctx context.Context,
    workloadInfo WorkloadInfo,
    placementPolicy *tmcv1alpha1.WorkloadPlacement,
) (*PlacementDecision, error) {
    
    e.mu.RLock()
    defer e.mu.RUnlock()
    
    // Check cache first
    cacheKey := e.buildCacheKey(workloadInfo, placementPolicy)
    if cached := e.placementCache.Get(cacheKey); cached != nil {
        klog.V(4).InfoS("Using cached placement decision", "key", cacheKey)
        return cached, nil
    }
    
    // Get available clusters for this workspace
    availableClusters, err := e.getAvailableClusters(ctx)
    if err != nil {
        return nil, fmt.Errorf("failed to get available clusters: %w", err)
    }
    
    if len(availableClusters) == 0 {
        return nil, fmt.Errorf("no available clusters in workspace %s", e.workspace)
    }
    
    // Filter clusters based on placement constraints
    candidateClusters, err := e.filterClusters(availableClusters, placementPolicy, workloadInfo)
    if err != nil {
        return nil, fmt.Errorf("failed to filter clusters: %w", err)
    }
    
    if len(candidateClusters) == 0 {
        return nil, fmt.Errorf("no clusters match placement constraints")
    }
    
    // Score and select clusters
    scoredClusters := e.scheduler.ScoreClusters(candidateClusters, workloadInfo, placementPolicy)
    selectedClusters := e.scheduler.SelectClusters(scoredClusters, placementPolicy.Spec.Strategy)
    
    decision := &PlacementDecision{
        TargetClusters: selectedClusters,
        Strategy:       placementPolicy.Spec.Strategy,
        Timestamp:      time.Now(),
        Reason:         e.buildDecisionReason(selectedClusters),
    }
    
    // Cache the decision
    e.placementCache.Set(cacheKey, decision)
    
    klog.V(2).InfoS("Made placement decision",
        "workload", workloadInfo.Name,
        "namespace", workloadInfo.Namespace,
        "strategy", decision.Strategy,
        "clusters", len(decision.TargetClusters))
    
    return decision, nil
}

// getAvailableClusters gets all healthy clusters in the workspace
func (e *PlacementEngine) getAvailableClusters(
    ctx context.Context,
) ([]*tmcv1alpha1.ClusterRegistration, error) {
    
    allClusters, err := e.clusterRegistrationLister.Cluster(e.workspace).List(labels.Everything())
    if err != nil {
        return nil, err
    }
    
    var availableClusters []*tmcv1alpha1.ClusterRegistration
    
    for _, cluster := range allClusters {
        // Only include healthy clusters
        if conditions.IsTrue(cluster, tmcv1alpha1.ClusterRegistrationReady) {
            availableClusters = append(availableClusters, cluster)
        }
    }
    
    return availableClusters, nil
}

// filterClusters filters clusters based on placement constraints
func (e *PlacementEngine) filterClusters(
    clusters []*tmcv1alpha1.ClusterRegistration,
    placement *tmcv1alpha1.WorkloadPlacement,
    workloadInfo WorkloadInfo,
) ([]*tmcv1alpha1.ClusterRegistration, error) {
    
    var filtered []*tmcv1alpha1.ClusterRegistration
    
    for _, cluster := range clusters {
        if e.clusterMeetsConstraints(cluster, placement, workloadInfo) {
            filtered = append(filtered, cluster)
        }
    }
    
    return filtered, nil
}

// clusterMeetsConstraints checks if a cluster meets placement constraints
func (e *PlacementEngine) clusterMeetsConstraints(
    cluster *tmcv1alpha1.ClusterRegistration,
    placement *tmcv1alpha1.WorkloadPlacement,
    workloadInfo WorkloadInfo,
) bool {
    
    // Check location selector
    if placement.Spec.LocationSelector != nil {
        selector, err := metav1.LabelSelectorAsSelector(placement.Spec.LocationSelector)
        if err != nil {
            klog.ErrorS(err, "Invalid location selector", "placement", placement.Name)
            return false
        }
        
        clusterLabels := labels.Set{"location": cluster.Spec.Location}
        if !selector.Matches(clusterLabels) {
            return false
        }
    }
    
    // Check capability requirements
    for _, req := range placement.Spec.CapabilityRequirements {
        if !e.clusterHasCapability(cluster, req) {
            return false
        }
    }
    
    // Check capacity requirements
    if !e.capacityManager.HasCapacity(cluster.Name, workloadInfo.ResourceRequirements) {
        return false
    }
    
    return true
}

// clusterHasCapability checks if cluster has required capability
func (e *PlacementEngine) clusterHasCapability(
    cluster *tmcv1alpha1.ClusterRegistration,
    requirement tmcv1alpha1.CapabilityRequirement,
) bool {
    
    for _, capability := range cluster.Spec.Capabilities {
        if capability.Type == requirement.Type {
            return capability.Available
        }
    }
    
    return !requirement.Required // If not required, absence is OK
}

// buildCacheKey builds a cache key for placement decisions
func (e *PlacementEngine) buildCacheKey(
    workloadInfo WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) string {
    return fmt.Sprintf("%s/%s:%s:%s",
        workloadInfo.Namespace,
        workloadInfo.Name,
        placement.Name,
        placement.ResourceVersion)
}

// buildDecisionReason builds a human-readable reason for the placement decision
func (e *PlacementEngine) buildDecisionReason(clusters []ClusterSelection) string {
    if len(clusters) == 0 {
        return "No suitable clusters found"
    }
    
    if len(clusters) == 1 {
        return fmt.Sprintf("Selected cluster %s (score: %d)",
            clusters[0].ClusterName, clusters[0].Score)
    }
    
    return fmt.Sprintf("Selected %d clusters based on placement strategy", len(clusters))
}

// WorkloadInfo contains information about a workload for placement
type WorkloadInfo struct {
    Name                 string
    Namespace            string
    Kind                 string
    ResourceRequirements ResourceRequirements
    Labels               map[string]string
    Annotations          map[string]string
}

// ResourceRequirements defines resource requirements for placement
type ResourceRequirements struct {
    CPU     int64 // millicores
    Memory  int64 // bytes
    Storage int64 // bytes
}

// PlacementCache methods
func (c *PlacementCache) Get(key string) *PlacementDecision {
    c.mu.RLock()
    defer c.mu.RUnlock()
    
    decision, exists := c.decisions[key]
    if !exists {
        return nil
    }
    
    // Check if expired
    if time.Since(decision.Timestamp) > c.ttl {
        delete(c.decisions, key)
        return nil
    }
    
    return decision
}

func (c *PlacementCache) Set(key string, decision *PlacementDecision) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    c.decisions[key] = decision
}

// Cleanup expired entries periodically
func (c *PlacementCache) StartCleanup(ctx context.Context) {
    ticker := time.NewTicker(time.Minute)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            c.cleanup()
        }
    }
}

func (c *PlacementCache) cleanup() {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    now := time.Now()
    for key, decision := range c.decisions {
        if now.Sub(decision.Timestamp) > c.ttl {
            delete(c.decisions, key)
        }
    }
}
```

### **Capacity Manager:**
```go
// pkg/tmc/placement/capacity.go
package placement

import (
    "context"
    "fmt"
    "sync"
    "time"
    
    "k8s.io/klog/v2"
    
    "github.com/kcp-dev/logicalcluster/v3"
    
    tmcv1alpha1 "github.com/kcp-dev/kcp/pkg/apis/tmc/v1alpha1"
    kcpclientset "github.com/kcp-dev/kcp/sdk/client/clientset/versioned/cluster"
    tmcv1alpha1informers "github.com/kcp-dev/kcp/sdk/client/informers/externalversions/tmc/v1alpha1"
)

// CapacityManager tracks cluster resource capacity and utilization
type CapacityManager struct {
    kcpClusterClient          kcpclientset.ClusterInterface
    clusterRegistrationLister tmcv1alpha1informers.ClusterRegistrationClusterLister
    
    // Capacity tracking
    clusterCapacities map[string]*ClusterCapacity
    capacityMutex     sync.RWMutex
    
    workspace logicalcluster.Name
}

// ClusterCapacity tracks cluster resource capacity
type ClusterCapacity struct {
    ClusterName string
    
    // Total capacity
    TotalCPU     int64 // millicores
    TotalMemory  int64 // bytes
    TotalStorage int64 // bytes
    
    // Allocated capacity
    AllocatedCPU     int64
    AllocatedMemory  int64
    AllocatedStorage int64
    
    // Available capacity (calculated)
    AvailableCPU     int64
    AvailableMemory  int64
    AvailableStorage int64
    
    // Utilization percentages
    CPUUtilization     float64
    MemoryUtilization  float64
    StorageUtilization float64
    
    // Health and performance metrics
    HealthScore      int64 // 0-100
    PerformanceScore int64 // 0-100
    
    LastUpdated time.Time
}

// NewCapacityManager creates a new capacity manager
func NewCapacityManager(
    kcpClusterClient kcpclientset.ClusterInterface,
    clusterRegistrationInformer tmcv1alpha1informers.ClusterRegistrationClusterInformer,
    workspace logicalcluster.Name,
) (*CapacityManager, error) {
    
    cm := &CapacityManager{
        kcpClusterClient:          kcpClusterClient,
        clusterRegistrationLister: clusterRegistrationInformer.Lister(),
        clusterCapacities:         make(map[string]*ClusterCapacity),
        workspace:                 workspace,
    }
    
    return cm, nil
}

// HasCapacity checks if a cluster has capacity for the given requirements
func (cm *CapacityManager) HasCapacity(
    clusterName string,
    requirements ResourceRequirements,
) bool {
    
    cm.capacityMutex.RLock()
    defer cm.capacityMutex.RUnlock()
    
    capacity, exists := cm.clusterCapacities[clusterName]
    if !exists {
        // If we don't have capacity info, assume it has capacity
        // This will be updated when capacity data is available
        return true
    }
    
    // Check if cluster has enough available capacity
    if capacity.AvailableCPU < requirements.CPU {
        return false
    }
    
    if capacity.AvailableMemory < requirements.Memory {
        return false
    }
    
    if capacity.AvailableStorage < requirements.Storage {
        return false
    }
    
    return true
}

// GetClusterCapacity returns capacity information for a cluster
func (cm *CapacityManager) GetClusterCapacity(clusterName string) *ClusterCapacity {
    cm.capacityMutex.RLock()
    defer cm.capacityMutex.RUnlock()
    
    if capacity, exists := cm.clusterCapacities[clusterName]; exists {
        return capacity.DeepCopy()
    }
    
    return nil
}

// UpdateClusterCapacity updates capacity information for a cluster
func (cm *CapacityManager) UpdateClusterCapacity(
    clusterName string,
    capacity *ClusterCapacity,
) {
    
    cm.capacityMutex.Lock()
    defer cm.capacityMutex.Unlock()
    
    capacity.LastUpdated = time.Now()
    cm.clusterCapacities[clusterName] = capacity
    
    klog.V(4).InfoS("Updated cluster capacity",
        "cluster", clusterName,
        "cpuUtil", capacity.CPUUtilization,
        "memUtil", capacity.MemoryUtilization)
}

// StartCapacityTracking starts periodic capacity tracking
func (cm *CapacityManager) StartCapacityTracking(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    klog.InfoS("Starting capacity tracking")
    
    for {
        select {
        case <-ctx.Done():
            klog.InfoS("Stopping capacity tracking")
            return
        case <-ticker.C:
            cm.updateAllCapacities(ctx)
        }
    }
}

// updateAllCapacities updates capacity for all clusters
func (cm *CapacityManager) updateAllCapacities(ctx context.Context) {
    clusters, err := cm.clusterRegistrationLister.Cluster(cm.workspace).List(labels.Everything())
    if err != nil {
        klog.ErrorS(err, "Failed to list cluster registrations")
        return
    }
    
    for _, cluster := range clusters {
        if err := cm.updateClusterCapacityFromStatus(cluster); err != nil {
            klog.ErrorS(err, "Failed to update cluster capacity", "cluster", cluster.Name)
        }
    }
}

// updateClusterCapacityFromStatus updates capacity from cluster status
func (cm *CapacityManager) updateClusterCapacityFromStatus(
    cluster *tmcv1alpha1.ClusterRegistration,
) error {
    
    // Extract capacity information from cluster status
    // This would typically come from cluster metrics or status reporting
    
    capacity := &ClusterCapacity{
        ClusterName: cluster.Name,
        
        // These would be populated from actual cluster metrics
        TotalCPU:     10000, // 10 CPUs * 1000 millicores
        TotalMemory:  32 * 1024 * 1024 * 1024, // 32GB
        TotalStorage: 1024 * 1024 * 1024 * 1024, // 1TB
        
        // These would be calculated from actual workload allocation
        AllocatedCPU:     3000, // Example values
        AllocatedMemory:  12 * 1024 * 1024 * 1024,
        AllocatedStorage: 100 * 1024 * 1024 * 1024,
    }
    
    // Calculate available capacity
    capacity.AvailableCPU = capacity.TotalCPU - capacity.AllocatedCPU
    capacity.AvailableMemory = capacity.TotalMemory - capacity.AllocatedMemory
    capacity.AvailableStorage = capacity.TotalStorage - capacity.AllocatedStorage
    
    // Calculate utilization percentages
    capacity.CPUUtilization = float64(capacity.AllocatedCPU) / float64(capacity.TotalCPU) * 100
    capacity.MemoryUtilization = float64(capacity.AllocatedMemory) / float64(capacity.TotalMemory) * 100
    capacity.StorageUtilization = float64(capacity.AllocatedStorage) / float64(capacity.TotalStorage) * 100
    
    // Calculate health and performance scores
    capacity.HealthScore = cm.calculateHealthScore(cluster)
    capacity.PerformanceScore = cm.calculatePerformanceScore(capacity)
    
    cm.UpdateClusterCapacity(cluster.Name, capacity)
    
    return nil
}

// calculateHealthScore calculates cluster health score based on conditions
func (cm *CapacityManager) calculateHealthScore(
    cluster *tmcv1alpha1.ClusterRegistration,
) int64 {
    
    // Check cluster conditions
    for _, condition := range cluster.Status.Conditions {
        if condition.Type == string(tmcv1alpha1.ClusterRegistrationReady) {
            if condition.Status == metav1.ConditionTrue {
                return 100 // Healthy
            } else {
                return 50 // Degraded
            }
        }
    }
    
    return 0 // Unknown/Unhealthy
}

// calculatePerformanceScore calculates performance score based on utilization
func (cm *CapacityManager) calculatePerformanceScore(capacity *ClusterCapacity) int64 {
    // Higher utilization reduces performance score
    avgUtilization := (capacity.CPUUtilization + capacity.MemoryUtilization) / 2
    
    if avgUtilization < 50 {
        return 100 // Low utilization = high performance
    } else if avgUtilization < 80 {
        return 70 // Medium utilization = good performance
    } else if avgUtilization < 95 {
        return 40 // High utilization = reduced performance
    } else {
        return 10 // Very high utilization = poor performance
    }
}

// DeepCopy creates a deep copy of ClusterCapacity
func (c *ClusterCapacity) DeepCopy() *ClusterCapacity {
    if c == nil {
        return nil
    }
    
    return &ClusterCapacity{
        ClusterName:        c.ClusterName,
        TotalCPU:           c.TotalCPU,
        TotalMemory:        c.TotalMemory,
        TotalStorage:       c.TotalStorage,
        AllocatedCPU:       c.AllocatedCPU,
        AllocatedMemory:    c.AllocatedMemory,
        AllocatedStorage:   c.AllocatedStorage,
        AvailableCPU:       c.AvailableCPU,
        AvailableMemory:    c.AvailableMemory,
        AvailableStorage:   c.AvailableStorage,
        CPUUtilization:     c.CPUUtilization,
        MemoryUtilization:  c.MemoryUtilization,
        StorageUtilization: c.StorageUtilization,
        HealthScore:        c.HealthScore,
        PerformanceScore:   c.PerformanceScore,
        LastUpdated:        c.LastUpdated,
    }
}
```

## üìä **PR 8: Performance Optimization & Advanced Algorithms (~700 lines)**

**Objective**: Add performance optimization and sophisticated placement algorithms

### **Files Created:**
```
pkg/tmc/placement/algorithms.go                 (~300 lines)
pkg/tmc/placement/performance.go                (~200 lines)
pkg/tmc/placement/affinity.go                   (~200 lines)
```

### **Advanced Algorithms:**
```go
// pkg/tmc/placement/algorithms.go
package placement

import (
    "fmt"
    "math"
    "sort"
    
    tmcv1alpha1 "github.com/kcp-dev/kcp/pkg/apis/tmc/v1alpha1"
)

// Scheduler implements advanced placement algorithms
type Scheduler struct {
    scoringFunctions map[string]ScoringFunction
    weights          ScoringWeights
}

// ScoringFunction calculates a score for a cluster
type ScoringFunction func(
    cluster *tmcv1alpha1.ClusterRegistration,
    capacity *ClusterCapacity,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64

// ScoringWeights defines weights for different scoring criteria
type ScoringWeights struct {
    Capacity     float64
    Health       float64
    Performance  float64
    Affinity     float64
    Location     float64
}

// NewScheduler creates a new scheduler with default scoring functions
func NewScheduler() *Scheduler {
    s := &Scheduler{
        scoringFunctions: make(map[string]ScoringFunction),
        weights: ScoringWeights{
            Capacity:    0.3,
            Health:      0.3,
            Performance: 0.2,
            Affinity:    0.1,
            Location:    0.1,
        },
    }
    
    // Register default scoring functions
    s.scoringFunctions["capacity"] = s.scoreByCapacity
    s.scoringFunctions["health"] = s.scoreByHealth
    s.scoringFunctions["performance"] = s.scoreByPerformance
    s.scoringFunctions["affinity"] = s.scoreByAffinity
    s.scoringFunctions["location"] = s.scoreByLocation
    
    return s
}

// ScoreClusters scores all candidate clusters
func (s *Scheduler) ScoreClusters(
    clusters []*tmcv1alpha1.ClusterRegistration,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) []ClusterSelection {
    
    var selections []ClusterSelection
    
    for _, cluster := range clusters {
        score := s.calculateClusterScore(cluster, workload, placement)
        
        selections = append(selections, ClusterSelection{
            ClusterName: cluster.Name,
            Score:       score,
            Reason:      s.buildScoringReason(cluster, score),
        })
    }
    
    // Sort by score (highest first)
    sort.Slice(selections, func(i, j int) bool {
        return selections[i].Score > selections[j].Score
    })
    
    return selections
}

// calculateClusterScore calculates overall score for a cluster
func (s *Scheduler) calculateClusterScore(
    cluster *tmcv1alpha1.ClusterRegistration,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64 {
    
    var totalScore float64
    
    // Get cluster capacity (would be injected in real implementation)
    capacity := &ClusterCapacity{
        ClusterName:      cluster.Name,
        HealthScore:      85,
        PerformanceScore: 90,
        CPUUtilization:   60.0,
        MemoryUtilization: 45.0,
    }
    
    // Apply all scoring functions with weights
    for name, scoringFunc := range s.scoringFunctions {
        score := scoringFunc(cluster, capacity, workload, placement)
        weight := s.getWeight(name)
        
        totalScore += float64(score) * weight
    }
    
    return int64(totalScore)
}

// getWeight returns the weight for a scoring function
func (s *Scheduler) getWeight(functionName string) float64 {
    switch functionName {
    case "capacity":
        return s.weights.Capacity
    case "health":
        return s.weights.Health
    case "performance":
        return s.weights.Performance
    case "affinity":
        return s.weights.Affinity
    case "location":
        return s.weights.Location
    default:
        return 0.1 // Default weight
    }
}

// scoreByCapacity scores based on available capacity
func (s *Scheduler) scoreByCapacity(
    cluster *tmcv1alpha1.ClusterRegistration,
    capacity *ClusterCapacity,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64 {
    
    if capacity == nil {
        return 50 // Default score when capacity unknown
    }
    
    // Score based on available capacity relative to requirements
    cpuScore := s.calculateResourceScore(
        capacity.AvailableCPU,
        workload.ResourceRequirements.CPU,
    )
    
    memoryScore := s.calculateResourceScore(
        capacity.AvailableMemory,
        workload.ResourceRequirements.Memory,
    )
    
    // Return average of CPU and memory scores
    return (cpuScore + memoryScore) / 2
}

// calculateResourceScore calculates score based on resource availability
func (s *Scheduler) calculateResourceScore(available, required int64) int64 {
    if required == 0 {
        return 100 // No requirements = full score
    }
    
    if available < required {
        return 0 // Not enough capacity
    }
    
    // Score based on how much overhead we have
    ratio := float64(available) / float64(required)
    
    if ratio >= 10 {
        return 100 // Lots of overhead
    } else if ratio >= 5 {
        return 80 // Good overhead
    } else if ratio >= 2 {
        return 60 // Some overhead
    } else {
        return 40 // Minimal overhead
    }
}

// scoreByHealth scores based on cluster health
func (s *Scheduler) scoreByHealth(
    cluster *tmcv1alpha1.ClusterRegistration,
    capacity *ClusterCapacity,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64 {
    
    if capacity != nil {
        return capacity.HealthScore
    }
    
    // Fallback to condition-based health scoring
    for _, condition := range cluster.Status.Conditions {
        if condition.Type == string(tmcv1alpha1.ClusterRegistrationReady) {
            if condition.Status == metav1.ConditionTrue {
                return 100
            } else {
                return 25
            }
        }
    }
    
    return 0 // Unknown health
}

// scoreByPerformance scores based on cluster performance
func (s *Scheduler) scoreByPerformance(
    cluster *tmcv1alpha1.ClusterRegistration,
    capacity *ClusterCapacity,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64 {
    
    if capacity != nil {
        return capacity.PerformanceScore
    }
    
    return 75 // Default performance score
}

// scoreByAffinity scores based on affinity rules
func (s *Scheduler) scoreByAffinity(
    cluster *tmcv1alpha1.ClusterRegistration,
    capacity *ClusterCapacity,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64 {
    
    // Check workload annotations for affinity preferences
    if affinityCluster, exists := workload.Annotations["tmc.kcp.io/preferred-cluster"]; exists {
        if affinityCluster == cluster.Name {
            return 100 // Strong affinity match
        } else {
            return 25 // Not preferred cluster
        }
    }
    
    // Check anti-affinity
    if antiAffinityCluster, exists := workload.Annotations["tmc.kcp.io/avoid-cluster"]; exists {
        if antiAffinityCluster == cluster.Name {
            return 0 // Strong anti-affinity
        }
    }
    
    return 50 // Neutral affinity
}

// scoreByLocation scores based on location preferences
func (s *Scheduler) scoreByLocation(
    cluster *tmcv1alpha1.ClusterRegistration,
    capacity *ClusterCapacity,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) int64 {
    
    // Check workload annotations for location preferences
    if preferredLocation, exists := workload.Annotations["tmc.kcp.io/preferred-location"]; exists {
        if preferredLocation == cluster.Spec.Location {
            return 100 // Location match
        } else {
            return 30 // Not preferred location
        }
    }
    
    return 50 // Neutral location score
}

// SelectClusters selects clusters based on strategy
func (s *Scheduler) SelectClusters(
    scoredClusters []ClusterSelection,
    strategy tmcv1alpha1.PlacementStrategy,
) []ClusterSelection {
    
    if len(scoredClusters) == 0 {
        return nil
    }
    
    switch strategy {
    case tmcv1alpha1.PlacementStrategyRoundRobin:
        return s.selectRoundRobin(scoredClusters)
    case tmcv1alpha1.PlacementStrategySpread:
        return s.selectSpread(scoredClusters)
    case tmcv1alpha1.PlacementStrategyAffinity:
        return s.selectByAffinity(scoredClusters)
    default:
        return s.selectRoundRobin(scoredClusters)
    }
}

// selectRoundRobin selects the highest scoring cluster
func (s *Scheduler) selectRoundRobin(clusters []ClusterSelection) []ClusterSelection {
    if len(clusters) > 0 {
        return []ClusterSelection{clusters[0]}
    }
    return nil
}

// selectSpread selects multiple clusters for spreading
func (s *Scheduler) selectSpread(clusters []ClusterSelection) []ClusterSelection {
    // Select up to 3 highest scoring clusters for spreading
    maxClusters := int(math.Min(3, float64(len(clusters))))
    return clusters[:maxClusters]
}

// selectByAffinity selects based on affinity scoring
func (s *Scheduler) selectByAffinity(clusters []ClusterSelection) []ClusterSelection {
    // For affinity strategy, prefer clusters with high affinity scores
    // This would implement more sophisticated affinity logic
    return s.selectRoundRobin(clusters)
}

// buildScoringReason builds a reason for the scoring decision
func (s *Scheduler) buildScoringReason(
    cluster *tmcv1alpha1.ClusterRegistration,
    score int64,
) string {
    if score >= 80 {
        return fmt.Sprintf("Excellent match (score: %d)", score)
    } else if score >= 60 {
        return fmt.Sprintf("Good match (score: %d)", score)
    } else if score >= 40 {
        return fmt.Sprintf("Fair match (score: %d)", score)
    } else {
        return fmt.Sprintf("Poor match (score: %d)", score)
    }
}
```

### **Performance Optimization:**
```go
// pkg/tmc/placement/performance.go
package placement

import (
    "context"
    "sync"
    "time"
    
    "k8s.io/klog/v2"
)

// PerformanceOptimizer handles performance optimization for placement decisions
type PerformanceOptimizer struct {
    // Caching
    decisionCache    *DecisionCache
    capacityCache    *CapacityCache
    
    // Batching
    batchProcessor   *BatchProcessor
    
    // Metrics
    metricsCollector *PlacementMetrics
    
    // Configuration
    config *PerformanceConfig
}

// PerformanceConfig contains performance optimization settings
type PerformanceConfig struct {
    CacheTTL              time.Duration
    BatchSize             int
    BatchTimeout          time.Duration
    MaxConcurrentRequests int
    EnablePrefetching     bool
}

// DecisionCache caches placement decisions for performance
type DecisionCache struct {
    cache map[string]*CachedDecision
    ttl   time.Duration
    mutex sync.RWMutex
}

// CachedDecision represents a cached placement decision
type CachedDecision struct {
    Decision  *PlacementDecision
    Timestamp time.Time
    Hits      int64
}

// BatchProcessor processes placement requests in batches
type BatchProcessor struct {
    requestQueue chan *PlacementRequest
    batchSize    int
    batchTimeout time.Duration
    processor    func([]*PlacementRequest) error
}

// PlacementRequest represents a placement request for batching
type PlacementRequest struct {
    WorkloadInfo    WorkloadInfo
    PlacementPolicy *tmcv1alpha1.WorkloadPlacement
    ResultChan      chan *PlacementResult
}

// PlacementResult represents the result of a placement request
type PlacementResult struct {
    Decision *PlacementDecision
    Error    error
}

// PlacementMetrics collects performance metrics
type PlacementMetrics struct {
    TotalRequests     int64
    CacheHits         int64
    CacheMisses       int64
    AverageLatency    time.Duration
    BatchedRequests   int64
    
    mutex sync.RWMutex
}

// NewPerformanceOptimizer creates a new performance optimizer
func NewPerformanceOptimizer(config *PerformanceConfig) *PerformanceOptimizer {
    if config == nil {
        config = &PerformanceConfig{
            CacheTTL:              5 * time.Minute,
            BatchSize:             10,
            BatchTimeout:          100 * time.Millisecond,
            MaxConcurrentRequests: 100,
            EnablePrefetching:     true,
        }
    }
    
    decisionCache := &DecisionCache{
        cache: make(map[string]*CachedDecision),
        ttl:   config.CacheTTL,
    }
    
    batchProcessor := &BatchProcessor{
        requestQueue: make(chan *PlacementRequest, config.MaxConcurrentRequests),
        batchSize:    config.BatchSize,
        batchTimeout: config.BatchTimeout,
    }
    
    return &PerformanceOptimizer{
        decisionCache:    decisionCache,
        batchProcessor:   batchProcessor,
        metricsCollector: &PlacementMetrics{},
        config:          config,
    }
}

// Start starts the performance optimizer
func (po *PerformanceOptimizer) Start(ctx context.Context) {
    klog.InfoS("Starting placement performance optimizer")
    
    // Start batch processor
    go po.batchProcessor.Start(ctx)
    
    // Start cache cleanup
    go po.decisionCache.StartCleanup(ctx)
    
    // Start metrics reporting
    go po.metricsCollector.StartReporting(ctx)
}

// OptimizePlacementRequest optimizes a placement request
func (po *PerformanceOptimizer) OptimizePlacementRequest(
    ctx context.Context,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
    placementEngine *PlacementEngine,
) (*PlacementDecision, error) {
    
    startTime := time.Now()
    defer func() {
        latency := time.Since(startTime)
        po.metricsCollector.RecordLatency(latency)
    }()
    
    po.metricsCollector.IncrementRequests()
    
    // Check cache first
    cacheKey := po.buildCacheKey(workload, placement)
    if cached := po.decisionCache.Get(cacheKey); cached != nil {
        po.metricsCollector.IncrementCacheHits()
        return cached.Decision, nil
    }
    
    po.metricsCollector.IncrementCacheMisses()
    
    // If batching is enabled and load is high, use batch processing
    if po.shouldBatch() {
        return po.processBatched(ctx, workload, placement, placementEngine)
    }
    
    // Process immediately
    decision, err := placementEngine.MakePlacementDecision(ctx, workload, placement)
    if err != nil {
        return nil, err
    }
    
    // Cache the result
    po.decisionCache.Set(cacheKey, decision)
    
    return decision, nil
}

// shouldBatch determines if request should be batched
func (po *PerformanceOptimizer) shouldBatch() bool {
    // Simple heuristic: batch if cache miss rate is high
    metrics := po.metricsCollector.GetMetrics()
    if metrics.TotalRequests < 100 {
        return false // Not enough data
    }
    
    missRate := float64(metrics.CacheMisses) / float64(metrics.TotalRequests)
    return missRate > 0.5 // Batch if miss rate > 50%
}

// processBatched processes request using batching
func (po *PerformanceOptimizer) processBatched(
    ctx context.Context,
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
    placementEngine *PlacementEngine,
) (*PlacementDecision, error) {
    
    request := &PlacementRequest{
        WorkloadInfo:    workload,
        PlacementPolicy: placement,
        ResultChan:      make(chan *PlacementResult, 1),
    }
    
    select {
    case po.batchProcessor.requestQueue <- request:
        // Request queued successfully
    case <-ctx.Done():
        return nil, ctx.Err()
    default:
        // Queue full, process immediately
        return placementEngine.MakePlacementDecision(ctx, workload, placement)
    }
    
    // Wait for result
    select {
    case result := <-request.ResultChan:
        return result.Decision, result.Error
    case <-ctx.Done():
        return nil, ctx.Err()
    }
}

// buildCacheKey builds a cache key for placement decisions
func (po *PerformanceOptimizer) buildCacheKey(
    workload WorkloadInfo,
    placement *tmcv1alpha1.WorkloadPlacement,
) string {
    return fmt.Sprintf("%s:%s:%s:%s",
        workload.Namespace,
        workload.Name,
        placement.Name,
        placement.ResourceVersion)
}

// DecisionCache methods
func (dc *DecisionCache) Get(key string) *CachedDecision {
    dc.mutex.RLock()
    defer dc.mutex.RUnlock()
    
    cached, exists := dc.cache[key]
    if !exists {
        return nil
    }
    
    // Check if expired
    if time.Since(cached.Timestamp) > dc.ttl {
        delete(dc.cache, key)
        return nil
    }
    
    cached.Hits++
    return cached
}

func (dc *DecisionCache) Set(key string, decision *PlacementDecision) {
    dc.mutex.Lock()
    defer dc.mutex.Unlock()
    
    dc.cache[key] = &CachedDecision{
        Decision:  decision,
        Timestamp: time.Now(),
        Hits:      0,
    }
}

func (dc *DecisionCache) StartCleanup(ctx context.Context) {
    ticker := time.NewTicker(time.Minute)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            dc.cleanup()
        }
    }
}

func (dc *DecisionCache) cleanup() {
    dc.mutex.Lock()
    defer dc.mutex.Unlock()
    
    now := time.Now()
    for key, cached := range dc.cache {
        if now.Sub(cached.Timestamp) > dc.ttl {
            delete(dc.cache, key)
        }
    }
}

// PlacementMetrics methods
func (pm *PlacementMetrics) IncrementRequests() {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    pm.TotalRequests++
}

func (pm *PlacementMetrics) IncrementCacheHits() {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    pm.CacheHits++
}

func (pm *PlacementMetrics) IncrementCacheMisses() {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    pm.CacheMisses++
}

func (pm *PlacementMetrics) RecordLatency(latency time.Duration) {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    
    // Simple moving average (would use more sophisticated approach in production)
    if pm.AverageLatency == 0 {
        pm.AverageLatency = latency
    } else {
        pm.AverageLatency = (pm.AverageLatency + latency) / 2
    }
}

func (pm *PlacementMetrics) GetMetrics() PlacementMetrics {
    pm.mutex.RLock()
    defer pm.mutex.RUnlock()
    return *pm
}

func (pm *PlacementMetrics) StartReporting(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            metrics := pm.GetMetrics()
            klog.InfoS("Placement performance metrics",
                "totalRequests", metrics.TotalRequests,
                "cacheHitRate", float64(metrics.CacheHits)/float64(metrics.TotalRequests)*100,
                "averageLatency", metrics.AverageLatency)
        }
    }
}
```

## ‚úÖ **Phase 4 Success Criteria**

### **Advanced Placement Compliance:**
1. **‚úÖ Sophisticated algorithms** - multi-factor scoring with capacity, health, performance
2. **‚úÖ Workspace integration** - placement decisions respect workspace boundaries
3. **‚úÖ Capacity management** - tracks cluster resources and prevents oversubscription
4. **‚úÖ Performance optimization** - caching, batching, efficient decision making
5. **‚úÖ Health-aware placement** - unhealthy clusters excluded automatically

### **Technical Validation:**
- Placement decisions consider multiple factors (capacity, health, affinity)
- Cluster capacity tracking prevents oversubscription
- Performance optimization reduces placement latency
- Workspace isolation is maintained in placement decisions
- Algorithm scoring produces reasonable placement decisions

### **Performance Benchmarks:**
- Placement decisions under 50ms for cached results
- Placement decisions under 200ms for uncached results
- Supports 1000+ placement decisions per second
- Cache hit rate above 80% for similar workloads
- Cluster capacity updates every 30 seconds

## üéØ **Phase 4 Outcome**

This phase establishes:
- **Production-ready placement algorithms** with multi-factor scoring
- **Cluster capacity management** preventing oversubscription
- **Performance optimization** for high-throughput scenarios
- **Workspace-aware placement** respecting KCP multi-tenancy
- **Foundation for enterprise features** in Phase 5

**Phase 4 provides sophisticated, production-ready placement capabilities that scale efficiently while maintaining KCP's architectural principles and workspace isolation.**