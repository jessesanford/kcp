// Copyright The KCP Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Package controller implements the TMC external controller foundation.
// This controller is designed to consume KCP TMC APIs via APIBinding and manage
// physical Kubernetes clusters for workload placement and execution.
package controller

import (
	"context"
	"fmt"
	"time"

	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/klog/v2"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/client-go/util/workqueue"

	kcpclientset "github.com/kcp-dev/kcp/sdk/client/clientset/versioned/cluster"
	"github.com/kcp-dev/logicalcluster/v3"
	"github.com/kcp-dev/kcp/pkg/reconciler/committer"
)

// TMC API Types - placeholder types until full TMC APIs are available
// These will be replaced by proper TMC API types in future PRs

// TMCClusterRegistrationSpec defines the desired state of cluster registration
type TMCClusterRegistrationSpec struct {
	// Location specifies the geographic or logical location of the cluster
	Location string `json:"location,omitempty"`
	
	// Capabilities describes what the cluster can do
	Capabilities []string `json:"capabilities,omitempty"`
	
	// KubeConfig reference for connecting to the cluster
	KubeConfigSecretName string `json:"kubeConfigSecretName,omitempty"`
}

// TMCClusterRegistrationStatus defines the observed state of cluster registration
type TMCClusterRegistrationStatus struct {
	// Conditions represent the latest available observations
	Conditions []metav1.Condition `json:"conditions,omitempty"`
	
	// Phase indicates the current lifecycle phase
	Phase string `json:"phase,omitempty"`
	
	// LastHealthCheck timestamp of last successful health check
	LastHealthCheck *metav1.Time `json:"lastHealthCheck,omitempty"`
	
	// ClusterInfo contains discovered information about the cluster
	ClusterInfo *ClusterInfo `json:"clusterInfo,omitempty"`
}

// ClusterInfo contains discovered information about a physical cluster
type ClusterInfo struct {
	// Version of the Kubernetes cluster
	Version string `json:"version,omitempty"`
	
	// NodeCount in the cluster
	NodeCount int `json:"nodeCount,omitempty"`
	
	// Region where the cluster is located
	Region string `json:"region,omitempty"`
}

// TMCClusterRegistrationResource wraps cluster registration for committer pattern
type TMCClusterRegistrationResource struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	
	Spec   TMCClusterRegistrationSpec   `json:"spec,omitempty"`
	Status TMCClusterRegistrationStatus `json:"status,omitempty"`
}

// DeepCopyObject implements runtime.Object
// In real implementation, this would be generated by controller-gen
func (r *TMCClusterRegistrationResource) DeepCopyObject() runtime.Object {
	return &TMCClusterRegistrationResource{} // Placeholder implementation
}



// ClusterRegistrationReconciler defines the interface for reconciling cluster registrations
type ClusterRegistrationReconciler interface {
	// Reconcile processes a cluster registration resource
	Reconcile(ctx context.Context, clusterName string) error
}

// ReconcilerWithCommit defines a reconciler that supports the committer pattern
type ReconcilerWithCommit interface {
	ClusterRegistrationReconciler
	// GetCommitter returns the committer function for resource updates
	GetCommitter() committer.CommitFunc[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus]
}

// ClusterRegistrationController manages physical cluster registration and health.
// This controller is responsible for:
// - Connecting to physical Kubernetes clusters
// - Performing health checks on registered clusters
// - Managing cluster capabilities and status
// - Preparing for TMC API consumption via APIBinding (Phase 1 integration)
type ClusterRegistrationController struct {
	// Core components
	queue workqueue.TypedRateLimitingInterface[string]

	// Committer for TMC ClusterRegistration resources
	commitClusterRegistration committer.CommitFunc[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus]

	// KCP client for future TMC API consumption
	kcpClusterClient kcpclientset.ClusterInterface

	// Physical cluster clients for health checking
	clusterClients map[string]kubernetes.Interface

	// TMC resource tracking
	clusterResources map[string]*committer.Resource[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus]

	// Configuration
	workspace    logicalcluster.Name
	resyncPeriod time.Duration
	workerCount  int

	// Health checking state
	clusterHealth map[string]*ClusterHealthStatus
}

// ClusterHealthStatus tracks the health of a physical cluster
type ClusterHealthStatus struct {
	// Name of the cluster
	Name string

	// LastCheck time of last health check
	LastCheck time.Time

	// Healthy indicates if the cluster is healthy
	Healthy bool

	// Error message if unhealthy
	Error string

	// NodeCount from the latest health check
	NodeCount int

	// Version of the Kubernetes cluster
	Version string
}

// NewClusterRegistrationController creates a new cluster registration controller.
// This controller provides the foundation for TMC cluster management and will
// integrate with KCP TMC APIs once they are available via APIBinding.
func NewClusterRegistrationController(
	kcpClusterClient kcpclientset.ClusterInterface,
	clusterConfigs map[string]*rest.Config,
	workspace logicalcluster.Name,
	resyncPeriod time.Duration,
	workerCount int,
) (*ClusterRegistrationController, error) {

	if len(clusterConfigs) == 0 {
		return nil, fmt.Errorf("at least one cluster configuration is required")
	}

	// Build cluster clients
	clusterClients := make(map[string]kubernetes.Interface)
	clusterHealth := make(map[string]*ClusterHealthStatus)
	clusterResources := make(map[string]*committer.Resource[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus])

	for name, config := range clusterConfigs {
		client, err := kubernetes.NewForConfig(config)
		if err != nil {
			return nil, fmt.Errorf("failed to create client for cluster %s: %w", name, err)
		}
		clusterClients[name] = client
		clusterHealth[name] = &ClusterHealthStatus{
			Name:    name,
			Healthy: false, // Start as unhealthy until first check
		}

		klog.V(2).InfoS("Configured cluster client", "cluster", name)
	}

	// Create the committer using KCP committer pattern
	// Since we can't use NewCommitter directly with our placeholder types,
	// we'll create a wrapper function that mimics the committer pattern behavior
	commitClusterRegistration := func(ctx context.Context, old, new *committer.Resource[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus]) error {
		klog.V(4).InfoS("Committing TMC ClusterRegistration resource", 
			"cluster", new.Name, 
			"workspace", workspace,
			"phase", new.Status.Phase)
		
		// In real implementation, this would call the actual TMC patcher
		// For now, we just log the operation
		return nil
	}

	c := &ClusterRegistrationController{
		queue: workqueue.NewTypedRateLimitingQueue(
			workqueue.DefaultTypedControllerRateLimiter[string]()),
		commitClusterRegistration: commitClusterRegistration,
		kcpClusterClient:         kcpClusterClient,
		clusterClients:           clusterClients,
		clusterResources:         clusterResources,
		workspace:                workspace,
		resyncPeriod:             resyncPeriod,
		workerCount:              workerCount,
		clusterHealth:            clusterHealth,
	}

	klog.InfoS("Created cluster registration controller",
		"workspace", workspace,
		"clusters", len(clusterConfigs),
		"resyncPeriod", resyncPeriod)

	return c, nil
}

// Reconcile implements ClusterRegistrationReconciler.Reconcile
func (c *ClusterRegistrationController) Reconcile(ctx context.Context, clusterName string) error {
	return c.syncCluster(ctx, clusterName)
}

// GetCommitter implements ReconcilerWithCommit.GetCommitter
func (c *ClusterRegistrationController) GetCommitter() committer.CommitFunc[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus] {
	return c.commitClusterRegistration
}

// Start runs the cluster registration controller.
// This method starts health checking of all configured clusters and prepares
// for future integration with KCP TMC APIs via APIBinding.
func (c *ClusterRegistrationController) Start(ctx context.Context) error {
	defer utilruntime.HandleCrash()
	defer c.queue.ShutDown()

	klog.InfoS("Starting ClusterRegistration controller",
		"workspace", c.workspace,
		"clusters", len(c.clusterClients))
	defer klog.InfoS("Shutting down ClusterRegistration controller")

	// Start periodic health checks
	go c.startHealthChecking(ctx)

	// Start workers
	for i := 0; i < c.workerCount; i++ {
		go wait.UntilWithContext(ctx, c.runWorker, time.Second)
	}

	// Initial health check for all clusters
	for clusterName := range c.clusterClients {
		c.queue.Add(clusterName)
	}

	<-ctx.Done()
	return nil
}

// runWorker is a long-running function that will continually call the
// processNextWorkItem function in order to read and process a message on the
// workqueue.
func (c *ClusterRegistrationController) runWorker(ctx context.Context) {
	for c.processNextWorkItem(ctx) {
	}
}

// processNextWorkItem will read a single work item off the workqueue and
// attempt to process it, by calling the syncHandler.
func (c *ClusterRegistrationController) processNextWorkItem(ctx context.Context) bool {
	obj, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(obj)

	clusterName := obj
	if err := c.syncCluster(ctx, clusterName); err != nil {
		utilruntime.HandleError(fmt.Errorf("error syncing cluster %s: %w", clusterName, err))
		c.queue.AddRateLimited(obj)
		return true
	}

	c.queue.Forget(obj)
	return true
}

// syncCluster performs health checking and TMC resource updates for a single cluster
func (c *ClusterRegistrationController) syncCluster(ctx context.Context, clusterName string) error {
	klog.V(4).InfoS("Syncing cluster", "cluster", clusterName)

	client, exists := c.clusterClients[clusterName]
	if !exists {
		return fmt.Errorf("cluster client not found: %s", clusterName)
	}

	// Perform health check
	healthy, err := c.performHealthCheck(ctx, clusterName, client)

	// Update health status
	c.clusterHealth[clusterName] = &ClusterHealthStatus{
		Name:      clusterName,
		LastCheck: time.Now(),
		Healthy:   healthy,
		Error: func() string {
			if err != nil {
				return err.Error()
			}
			return ""
		}(),
	}

	// Update TMC resource using committer pattern
	if err := c.updateTMCResource(ctx, clusterName, healthy, err); err != nil {
		return fmt.Errorf("failed to update TMC resource: %w", err)
	}

	if healthy {
		klog.V(2).InfoS("Cluster health check passed", "cluster", clusterName)
	} else {
		klog.V(2).InfoS("Cluster health check failed", "cluster", clusterName, "error", err)
	}

	return nil
}

// updateTMCResource updates the TMC ClusterRegistration resource using the committer pattern
func (c *ClusterRegistrationController) updateTMCResource(ctx context.Context, clusterName string, healthy bool, healthErr error) error {
	// Get or create the TMC resource
	oldResource, exists := c.clusterResources[clusterName]
	if !exists {
		// Create initial resource
		now := metav1.Now()
		oldResource = &committer.Resource[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus]{
			ObjectMeta: metav1.ObjectMeta{
				Name: clusterName,
				Annotations: map[string]string{
					"tmc.kcp.io/workspace": string(c.workspace),
				},
			},
			Spec: TMCClusterRegistrationSpec{
				Location: "unknown", // This would come from cluster configuration
			},
			Status: TMCClusterRegistrationStatus{
				Phase:           "Initializing",
				LastHealthCheck: &now,
				Conditions:      []metav1.Condition{},
			},
		}
		c.clusterResources[clusterName] = oldResource
	}

	// Create updated resource
	newResource := &committer.Resource[TMCClusterRegistrationSpec, TMCClusterRegistrationStatus]{
		ObjectMeta: oldResource.ObjectMeta,
		Spec:       oldResource.Spec,
		Status:     oldResource.Status,
	}

	// Update status based on health check
	now := metav1.Now()
	newResource.Status.LastHealthCheck = &now

	if healthy {
		newResource.Status.Phase = "Ready"
		// Update or add Ready condition
		readyCondition := metav1.Condition{
			Type:               "Ready",
			Status:             metav1.ConditionTrue,
			LastTransitionTime: now,
			Reason:             "HealthCheckPassed",
			Message:            "Cluster is healthy and ready",
		}
		newResource.Status.Conditions = c.updateCondition(newResource.Status.Conditions, readyCondition)
	} else {
		newResource.Status.Phase = "NotReady"
		// Update or add Ready condition
		readyCondition := metav1.Condition{
			Type:               "Ready",
			Status:             metav1.ConditionFalse,
			LastTransitionTime: now,
			Reason:             "HealthCheckFailed",
			Message:            fmt.Sprintf("Health check failed: %v", healthErr),
		}
		newResource.Status.Conditions = c.updateCondition(newResource.Status.Conditions, readyCondition)
	}

	// Update cluster info if available from health status
	if healthStatus, ok := c.clusterHealth[clusterName]; ok {
		newResource.Status.ClusterInfo = &ClusterInfo{
			Version:   healthStatus.Version,
			NodeCount: healthStatus.NodeCount,
		}
	}

	// Use committer to patch the resource
	if err := c.commitClusterRegistration(ctx, oldResource, newResource); err != nil {
		return fmt.Errorf("failed to commit cluster registration update: %w", err)
	}

	// Update our local cache
	c.clusterResources[clusterName] = newResource

	return nil
}

// updateCondition updates or adds a condition to the condition list
func (c *ClusterRegistrationController) updateCondition(conditions []metav1.Condition, newCondition metav1.Condition) []metav1.Condition {
	for i, condition := range conditions {
		if condition.Type == newCondition.Type {
			// Update existing condition
			conditions[i] = newCondition
			return conditions
		}
	}
	// Add new condition
	return append(conditions, newCondition)
}

// performHealthCheck tests cluster connectivity and basic functionality
func (c *ClusterRegistrationController) performHealthCheck(ctx context.Context, clusterName string, client kubernetes.Interface) (bool, error) {
	// Test 1: List nodes to verify API server connectivity
	nodeList, err := client.CoreV1().Nodes().List(ctx, metav1.ListOptions{Limit: 1})
	if err != nil {
		return false, fmt.Errorf("failed to list nodes: %w", err)
	}

	// Test 2: Get cluster version
	version, err := client.Discovery().ServerVersion()
	if err != nil {
		return false, fmt.Errorf("failed to get server version: %w", err)
	}

	// Update health status with additional info
	if health, exists := c.clusterHealth[clusterName]; exists {
		health.NodeCount = len(nodeList.Items)
		health.Version = version.String()
	}

	klog.V(4).InfoS("Cluster health check details",
		"cluster", clusterName,
		"nodes", len(nodeList.Items),
		"version", version.String())

	return true, nil
}

// startHealthChecking starts periodic health checking for all clusters
func (c *ClusterRegistrationController) startHealthChecking(ctx context.Context) {
	ticker := time.NewTicker(c.resyncPeriod)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// Queue health checks for all clusters
			for clusterName := range c.clusterClients {
				c.queue.Add(clusterName)
			}
		}
	}
}

// GetClusterHealth returns the current health status of a cluster
func (c *ClusterRegistrationController) GetClusterHealth(clusterName string) (*ClusterHealthStatus, bool) {
	health, exists := c.clusterHealth[clusterName]
	if !exists {
		return nil, false
	}

	// Return a copy to avoid race conditions
	return &ClusterHealthStatus{
		Name:      health.Name,
		LastCheck: health.LastCheck,
		Healthy:   health.Healthy,
		Error:     health.Error,
		NodeCount: health.NodeCount,
		Version:   health.Version,
	}, true
}

// GetAllClusterHealth returns health status for all clusters
func (c *ClusterRegistrationController) GetAllClusterHealth() map[string]*ClusterHealthStatus {
	result := make(map[string]*ClusterHealthStatus)

	for name, health := range c.clusterHealth {
		result[name] = &ClusterHealthStatus{
			Name:      health.Name,
			LastCheck: health.LastCheck,
			Healthy:   health.Healthy,
			Error:     health.Error,
			NodeCount: health.NodeCount,
			Version:   health.Version,
		}
	}

	return result
}

// IsHealthy returns true if all clusters are healthy
func (c *ClusterRegistrationController) IsHealthy() bool {
	for _, health := range c.clusterHealth {
		if !health.Healthy {
			return false
		}
	}
	return true
}
